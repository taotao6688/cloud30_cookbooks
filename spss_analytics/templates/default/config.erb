root#/****************************************************************
#* IBM Confidential
#*
#* OCO Source Materials
#*
#* IBM SPSS Products: Analytic Server
#*
#* (C) Copyright IBM Corp. 2012-2013
#*
#* The source code for this program is not published or otherwise
#* divested of its trade secrets, irrespective of what has been
#* deposited with the U.S. Copyright Office.
#*****************************************************************/

# Used for clustered deployment only. Each cluster node should have
# a unique server.id
server.id=balancer.http1

# The folder where the analytics results will be stored.
root.folder.path=/user/<%= node['spss_analytics']['biginsights_user']%>/analytic-root/analytic-workspace

#IBM SPSS Analytic Server temporary directory 
#as.temp.folder=/.temp

# The folder where plugable AE modules will be read
external.modules.path=modules

# The folder where executions history information will be written
# This is relative to the 'root.folder.path' property 
executions.history.path=.history

# The folder where admin UI is stored
web.home=ui

#Analytic-Engine process settings:
ae.jvm.settings=-Xms256m -Xmx2048m -server -Das.child.proc.group=1 -cp apps/AE_BOOT.war/WEB-INF/lib/*
ae.args=-maxconnections 10
ae.pool.size=2
ae.start.port=1200
ae.queue.ttl.seconds=1800

# Component framework settings
component.framework.bin.path=configuration
component.framework.temp.path=configuration/tmp
component.framerwork.max.threads=2
component.framework.max.cache=64

# Analytic framework settings
af.temp.path=configuration/tmp
af.components.path=configuration/af-components
# Determine if AE would yield an error when AF throws a warn message. Default it's false.
# true - yield an error when there is a warn message.
# false - only report the warn message.
af.warn.messages.as.errors=false
af.io.cache.size=33554432

# Number of records that can be cached in memory in interative operations.
# Default = -1 - Auto determine
records.cache.size=-1

# Number of large branches allow to attempt MSJ.
# Default = 2
join.large.branch.max=2

# The timeout when a waiting Comet connection will receive a response from server 
# to revitalize the physical connection. Expressed in seconds
comet.timeout = 120

# The lifetime of a queued comet message. After that it will be removed from queue.
# Expressed in seconds
comet.queued.message.validity=1800

# The maximum interval where the session is valid without any activity. Expressed in seconds.
session.max.inactivity.time=14400

# The number of minutes to delay between each background cleanup run
cleanup.delay=20

# The size of the thread pool used by the Analytic Engine
orchestrator.thread.pool=10

# The default administrator username
admin.username=<%= node['spss_analytics']['admin_username']%>

# If true, all executions are sent for distributed computation, even with small data sets  
small.data.distributed=false

# Preferred auth scheme sent in server response 
# Possible values:
# - basic (HTTP BASIC - set to default)
# - digest (HTTP DIGEST)
preferred.auth.scheme=basic
auth.realm=Secured Area


# The encryption provider settings
encryption.keystore=analytic_server.keystore
# This is a Base 64 encoded string of encrypted bytes containing the keystore password.
# It must be generated by running com.spss.ae.encryption.provider.EncryptKeystorePassword.
encryption.keystore.password=FEFFUy9FQ0IvUEtDUzVQYWRkaW5nADJyeHJu/jQmVyB+sO8siu4=

#Jndi name for database access
jndi.aedb=jdbc/aeds
jndi.aedb.url=jdbc:derby://localhost:9114/aedb
jndi.aedb.driver=org.apache.derby.jdbc.ClientDriver
jndi.aedb.pool=10


# Expressed in seconds. If this period exceeds the auth validation fails
# even if the calculated digest matches. Thus a new challenge is sent to client.
# This is used only if preferred.auth.scheme=digest
md5.nonce.validity=20

# Buffers settings. Sizes are in bytes.
read.buffer.size=32768
write.buffer.size=32768
pagination.items.count=200

# HIVE parameters - by default disabled ----------------------------------------------
# Uncomment the following two lines with hive.metastore.* parameters to enable HCatalog support. 
# The hive.metastore.uris should be in the following form thrift://hostname:portnum 
# where hostname is the name of the machine hosting the Thrift server, 
# and portnum is the port number used in the HCatalog installation script. 
#hive.metastore.local=false
#hive.metastore.uris=thrift://hostname:portnum

# Kerberos authentication parameters - by default disabled ---------------------------
# Uncomment the following lines for Kerberos support.
# hadoop.security.authentication=kerberos
# dfs.namenode.kerberos.principal=<Name node kerberos principle. ex: hdfs/hadoop.as.com@AS.COM>
# mapreduce.jobtracker.kerberos.principal=<Job tracker node kerberos principle. ex: mapred/hadoop.as.com@AS.COM>
# java.security.krb5.conf=/etc/krb5.conf


# HADOOP parameters - by default disabled ----------------------------------------------
hdfs.namenode.url=hdfs://<%= node['spss_analytics']['biginsights_ip']%>:<%= node['spss_analytics']['biginsights_namenode_port']%>/user/<%= node['spss_analytics']['biginsights_user']%>/analytic-root
hdfs.user=<%= node['spss_analytics']['biginsights_user']%>
hdfs.password=FEFFUy9FQ0IvUEtDUzVQYWRkaW5nABDDG4WfEcwM75nHmSoPYEw=
hdfs.classpath.folder=/user/<%= node['spss_analytics']['biginsights_user']%>/analytic-root/classpath
mapred.job.tracker=<%= node['spss_analytics']['biginsights_ip']%>:<%= node['spss_analytics']['biginsights_jobnode_port']%>
mapred.child.java.opts=-Xms256m -Xmx1024m -server -XX:+UseParNewGC
mapred.job.reuse.jvm.num.tasks=1
mapred.reduce.max.attempts=2
mapred.map.max.attempts=2
mapred.map.tasks.speculative.execution=false
mapred.reduce.tasks.speculative.execution=false
mapred.reduce.slowstart.completed.maps=0.2
# This can be used for tuning performance 
# mapred.tasktracker.reduce.tasks.maximum=2
io.sort.mb=100
mapred.task.timeout=18000000
local.ensembler.pool=1
mapred.ensembling.limit=16
split.reducers=-1
split.sort.mb=128
mapred.max.map.failures.percent=0
mapred.max.reduce.failures.percent=0
#---------------------------------------------------------------------------------------


#---------------------------------------------------------------------------------------
# AE modules when running locally
#---------------------------------------------------------------------------------------
ae.modules=restframework@local,\
 objectstore,\
 jndidb,\
 securityprovidermanager@local,\
 componentframework@remote,\
 scheduler,\
 http@local,\
 dbpersistence,\
 memsessionstore@local,\
 hdfsfilesystem,\
 fsprojectversioning,\
 notification,\
 monitoring,\
 filereporting,\
 defaultenvselector@remote,\
 orchestrator,\
 localClient,\
 aslfunctionsprovider@remote,\
 wssecurityprovider@local,\
 restanalyticapi@local,\
 restfileapi@local,\
 restnativefileapi@local,\
 restdatasourceapi@local,\
 restprojectapi@local,\
 restsecurityapi@local,\
 restcontainerapi@local,\
 restadmin@local,\
 fileupload@local,\
 validation,\
 fshistorystore,\
 hadoopmapreduce@remote,\
 inputsplitbuilders@remote,\
 mapreducepsm@remote,\
 mapreducesplitfile@remote,\
 mapreducedataparallel@remote,\
 mapreducesort@remote,\
 mapreduceunion@remote,\
 mapreducejoin@remote,\
 mapreducetaskparallel@remote,\
 smartmodeler@remote,\
 dataparser,\
 regexparser,\
 binparser,\
 delimitedparser,\
 fixedparser,\
 excelparser,\
 fsdataconverterstore,\
 dbreadprovider@remote,\
 sqltransformationenv@remote,\
 encryptionprovider,\
 binningexecutionshell@remote,\
 executiondaemon@local
 

